{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T11:17:12.366574Z",
     "start_time": "2021-10-24T11:17:11.451592Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import (StandardScaler,\n",
    "                                   Normalizer,\n",
    "                                   LabelEncoder\n",
    "                                  )\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     GridSearchCV\n",
    "                                    )\n",
    "from sklearn.metrics import (confusion_matrix,\n",
    "                             classification_report,\n",
    "                             accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score\n",
    "                            )\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import (check_array, \n",
    "                                      check_is_fitted, \n",
    "                                      check_X_y,\n",
    "                                      _check_sample_weight\n",
    "                                     )\n",
    "\n",
    "from sklearn.base import (BaseEstimator, \n",
    "                          TransformerMixin, \n",
    "                          clone\n",
    "                         )\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T03:03:48.750681Z",
     "start_time": "2021-10-24T03:03:48.730188Z"
    }
   },
   "outputs": [],
   "source": [
    "class svm(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    test_size = 0.2\n",
    "    m_vals = 0.05\n",
    "    \n",
    "    def __init__(self,\n",
    "                 estimator = \"SVC\"\n",
    "                 \n",
    "                 kernel:Optional[str] = \"rbf\",\n",
    "                 C:Optional[float] = 1e3,\n",
    "                 \n",
    "                 learn_rate:float = 0.001,\n",
    "                 tol:float = 0.05,\n",
    "                 batch_size:int = 1,\n",
    "                 n_epochs:int = 1000,\n",
    "                 decay:float = 1, \n",
    "                 random_number:Optional[float] = None,\n",
    "                 visualisation = True):\n",
    "        \n",
    "        BaseEstimator.__init__(self)\n",
    "        TransformerMixin.__init__(self)\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.kernel = self._kernel_type(kernel, **kwargs)\n",
    "        \n",
    "        self.C = C\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.decay = decay\n",
    "        self.random_number = random_number if random_number is not None else np.random.randint(0,100,size=1)\n",
    "        \n",
    "        self.visualisation = visualisation\n",
    "        self.colors = {1:\"r\", -1:\"b\"}\n",
    "        if self.visualisation:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1,1,1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "             \n",
    "        \n",
    "        \n",
    "    def missing_values(self, X:np.ndarray, threshold:int = None, verbose = False):\n",
    "        \"\"\"Step 0: Data pre-processing. Missing values.\"\"\"\n",
    "        if threshold:\n",
    "            m_vals_pct = ([len(isnull(X[:,i])) for i in range (X.shape[1])]/X.shape[0])\n",
    "            to_del = [i for i,v in enumerate(m_vals_pct) if v >= threshold]\n",
    "            if verbose:\n",
    "                print(\"del {}.\".format(to_del))\n",
    "            new_cols = [i for i in range(X.shape[1]) if i not in to_del]\n",
    "            X = X[:,new_cols]\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _estimator(self, n:str)\n",
    "        est = {\n",
    "            \"linear\": LinearRegression(),\n",
    "            \"svc\": SVC(kernel=self.kernel, C = self.C)\n",
    "        }\n",
    "    \n",
    "        return est[n]\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    def _kernel_type(self, kernel:str = None, **kwargs):\n",
    "        \"\"\"Kernel is a hyperparameter and is selected by the researcher.\"\"\"\n",
    "        \n",
    "        if self.estimator == \"SVC\":\n",
    "        \n",
    "            if kernel:\n",
    "                if isinstance(kernel, str) and kernel is not None:\n",
    "                \n",
    "                    if kernel == \"linear\":\n",
    "                        def linear_kernel():\n",
    "                            return lambda X,y: np.dot(X,y.T)\n",
    "        \n",
    "                    if kernel == \"poly\":\n",
    "                        def _polynomial_kernel(bias = 0, power = 2):\n",
    "                            return lambda X,y: (self.gamma * np.dot(X,y)+bias)**power\n",
    "        \n",
    "                    if kernel == \"rbf\":\n",
    "                        def _rbf_kernel():\n",
    "                            return lambda X_i,y_i: np.exp(-self.gamma * np.dot(X_i-y_i, X_i-y_i))  \n",
    "        \n",
    "            kernel_mapping = {\n",
    "                \"linear\": _linear_kernel,\n",
    "                \"poly\": _polynomial_kernel,\n",
    "                \"rbf\": _rbf_kernel\n",
    "            }\n",
    "            \n",
    "            if kernel not in {None,\"linear\", \"poly\", \"rbf\"}:\n",
    "                raise ValueError(f\"{self.kernel} kernel not recognised.\")\n",
    "\n",
    "        return \n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def loss_function(self):\n",
    "        \"\"\"The below returns the loss function for the stochastic gradient descent.\"\"\"\n",
    "        # The input to this function is the predicted output and the actual output.\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def label_encoding(self, y:np.ndarray):\n",
    "        l = LabelEncoder()\n",
    "        y = l.fit_transform(y)\n",
    "        return y\n",
    "        \n",
    "        \n",
    "    def train_test_split(self, X_pca:np.ndarray, y:np.ndarray):    \n",
    "        \"\"\"Step 3: Data pre-processing. Split the dataset into train and test set.\"\"\"\n",
    "        \n",
    "        X = X.values.reshape(-1,1)\n",
    "        y = y.values.reshape(-1,1)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size = svm.test_size, random_state = self.random_number, shuffle = True, stratify = y)\n",
    "        \n",
    "        X_train.shape, y_train.shape\n",
    "        X_test.shape, y_test.shape\n",
    "        \n",
    "        # Convert to float64\n",
    "        X_train, X_test = np.array(X_train,dtype = np.float64), np.array(X_test,dtype = np.float64) \n",
    "        \n",
    "        # reshape\n",
    "        n_samples, n_features = X.shape\n",
    "        X_train, X_test = X_train.reshape(-1, n_features), X_test.reshape(-1, n_features) \n",
    "        \n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    @decorator_function\n",
    "    def feature_scaling(self):\n",
    "        \"\"\"Step 4: Data pre-processing. Feature scaling.\"\"\"\n",
    "        \n",
    "        X_train, X_test = train_test_split()\n",
    "        \n",
    "        # Standardization is the process of scaling data so that they have a mean value of 0 and a standard deviation of 1\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # fit_transform on X_train but transform on X_test\n",
    "        X_train = scaler.fit_transform(X_train.values.reshape(-1,1))\n",
    "        X_test = scaler.transform(X_test.values.reshape(-1,1))\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def feature_extraction(self, var_retained:float = .95, np:bool = False):\n",
    "        \"\"\"Step 5: Data pre-processing. Principal Component Analysis.\"\"\"\n",
    "        pca = PCA(var_retained) # choose n_components such that 95% variance remains explained\n",
    "        \n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        \n",
    "        if len(X_train_pca.shape[1]) = len(X_train.shape[1]) and isinstance(X_train_pca, np.float64):\n",
    "            raise ValueError(\n",
    "                \"%d n_features in X_train_pca is not less than %d n_features in X_train.\" \n",
    "                    (len(X_train_pca.shape[1]), len(X_train.shape[1]))\n",
    "            )\n",
    "                                                                    \n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        \n",
    "        if np:\n",
    "            # calculate cov matrix\n",
    "            cov_matrix = np.cov(X_train.T)\n",
    "            eigen_values, eigen_vectors = np.linalg.eig(cov_matrix)\n",
    "            \n",
    "            \n",
    "            # calculating explained variance on each component\n",
    "            var_expl = [i/(sum(eigen_values))*100 for i in eigen_values]\n",
    "            \n",
    "            # identifying components that explain at least 95% variance\n",
    "            cum_var_expl = np.cumsum(var_expl)\n",
    "            \n",
    "    \n",
    "        return X_train, X_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X_train:Union[np.ndarray, pd.DataFrame], y_train:Union[np.ndarray, pd.DataFrame], sample_weight = 0.1):\n",
    "        \"\"\"Step 6: Model-fitting.\"\"\"\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "        \n",
    "            X_train, y_train = train_test_split()\n",
    "            n_samples, n_features = X.shape\n",
    "        \n",
    "            # if gamma is not specified in init, it is specified as\n",
    "            if not self.gamma:\n",
    "                self.gamma = 1/(n_features*X.var())\n",
    "        \n",
    "            if X_train.shape[1] != n_features:\n",
    "                raise ValueError(\"{} != {}\".format(X_train.shape[1], self.n_features))\n",
    "            elif X_train.shape[1] < 2:\n",
    "                raise ValueError(\"cannot fit model with {} features.\".format(X_train.shape[1]))\n",
    "            \n",
    "        \n",
    "            # Checks X and y for consistent length, enforces X to be 2D and y 1D. \n",
    "            # By default, X is checked to be non-empty and containing only finite values. \n",
    "            # Standard input checks are also applied to y, such as checking that y does not have np.nan or np.inf targets.\n",
    "            X, y = check_X_y(X, \n",
    "                             y, \n",
    "                             force_all_finite=False) # accepts np.nan in X\n",
    "        \n",
    "        \n",
    "            # By default, the input is checked to be a non-empty 2D array containing only finite values.\n",
    "            X = check_array(X, ensure_2d=True, ensure_min_samples=1, ensure_min_features=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            self.est = self._estimator(self.estimator)\n",
    "            self.est.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.n_iters):\n",
    "            self.classes_ = unique_labels(y)\n",
    "        \n",
    "        \n",
    "                   \n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                self.feature_names = X.columns.to_list()\n",
    "\n",
    "            # if feature_names is something other than pd.Dataframe         \n",
    "            elif self.feature_names is not None:\n",
    "                self.feature_names = list(self.feature_names)\n",
    "\n",
    "            else:\n",
    "                self.feature_names = None\n",
    "            \n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        y_ = np.where(y <= 0, -1,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        n_iters = int(n_iters)\n",
    "        if n_iters <= 0:\n",
    "            raise ValueError(\"{} must be greater than zero\".format(n_iters))\n",
    "        \n",
    "        for epoch in range(1, self.n_epochs):\n",
    "            \n",
    "            parameters = 0 # initialised as a vector of random values for the coefficient\n",
    "            \n",
    "            initial_learn_rate = 0.1\n",
    "            learn_rate = initial_learn_rate * (1/(1+self.decay*epoch))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for instance in range(X.shape[0]):\n",
    "                \n",
    "# parameter = parameter - (self.learning rate * gradient (derivative of cost function))\n",
    "                \n",
    "        if np.all(np.abs(diff) <= tol):\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, X_test:np.array, y = None):\n",
    "        \"\"\"Step 7: Model prediction.\"\"\"\n",
    "        if isinstance(X_test, np.array) and not y:\n",
    "            check_is_fitted(self, msg=\"is_fitted\")          \n",
    "        \n",
    "        \n",
    "        \n",
    "        np.sign(np.dot(np.array(features), self.w)+self.b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.base_regressor.predict(X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def evaluation(self):\n",
    "        \"\"\"The following function evaluates how well the model performs on the test data. Step 6: model evaluation.\"\"\"\n",
    "        print(\"\\nThe Classifier Accuracy Score is {:.2f}\\n\".format(clf.score(X_test, y_test)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_params(self, deep = True):\n",
    "        \"\"\"\n",
    "        The below function returns parameter values.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"C\": self.C,\n",
    "            \"kernel\": self.kernel\n",
    "            \"epsilon\": self.epsilon\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for param, val in params.items():\n",
    "            setattr(self, param, val)\n",
    "        return self    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def gridSearchCV(self):\n",
    "        param_grid = {\n",
    "            \"C\": [1,10,100,1000,10000],\n",
    "            \"gamma\": [1,0.1,0.01,0.001,0.00001],\n",
    "            \"kernel\": [\"linear\", \"poly\", \"rbf\"],\n",
    "            \"class_weight\": [\"balanced\", None]\n",
    "        }\n",
    "        \n",
    "        search = GridSearchCV(estimator = svm, \n",
    "                             param_grid = param_grid, \n",
    "                             cv = 5, # determines cross-validation splitting strategy, int, specify number of folds in StratifiedKfold\n",
    "                             verbose = 1, # control verbosity, the higher the more messages\n",
    "                             refit = True, # refit an estimator using the best found params on data\n",
    "                             scoring = accuracy)\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        y_pred = search.predict(X_test)\n",
    "        \n",
    "        print(\"Test Accuracy: {}.\".format(accuracy_score(y_test, y_pred)))\n",
    "        print(\"Best Params: {}.\".format(model.best_params_))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def main():\n",
    "        gspc = pd.read_csv(\"^GSPC.csv\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        m = svm()\n",
    "        \n",
    "        m.train_test_split()\n",
    "        m.feature_scaling()\n",
    "        m.feature_extraction()\n",
    "        \n",
    "        m.fit()\n",
    "        m.predict()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>4369.229980</td>\n",
       "      <td>4309.870117</td>\n",
       "      <td>4309.870117</td>\n",
       "      <td>4345.720215</td>\n",
       "      <td>2967400000</td>\n",
       "      <td>4345.720215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>4365.569824</td>\n",
       "      <td>4290.490234</td>\n",
       "      <td>4319.569824</td>\n",
       "      <td>4363.549805</td>\n",
       "      <td>3219590000</td>\n",
       "      <td>4363.549805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>2021-10-07</td>\n",
       "      <td>4429.970215</td>\n",
       "      <td>4383.729980</td>\n",
       "      <td>4383.729980</td>\n",
       "      <td>4399.759766</td>\n",
       "      <td>3096080000</td>\n",
       "      <td>4399.759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>4412.020020</td>\n",
       "      <td>4386.220215</td>\n",
       "      <td>4406.509766</td>\n",
       "      <td>4391.339844</td>\n",
       "      <td>2401890000</td>\n",
       "      <td>4391.339844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>4415.879883</td>\n",
       "      <td>4360.589844</td>\n",
       "      <td>4385.439941</td>\n",
       "      <td>4361.189941</td>\n",
       "      <td>2580000000</td>\n",
       "      <td>4361.189941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>2021-10-12</td>\n",
       "      <td>4374.890137</td>\n",
       "      <td>4342.089844</td>\n",
       "      <td>4368.310059</td>\n",
       "      <td>4350.649902</td>\n",
       "      <td>2608150000</td>\n",
       "      <td>4350.649902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2965</th>\n",
       "      <td>2021-10-13</td>\n",
       "      <td>4372.870117</td>\n",
       "      <td>4329.919922</td>\n",
       "      <td>4358.009766</td>\n",
       "      <td>4363.799805</td>\n",
       "      <td>2926460000</td>\n",
       "      <td>4363.799805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>2021-10-14</td>\n",
       "      <td>4439.729980</td>\n",
       "      <td>4386.750000</td>\n",
       "      <td>4386.750000</td>\n",
       "      <td>4438.259766</td>\n",
       "      <td>2642920000</td>\n",
       "      <td>4438.259766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>4475.819824</td>\n",
       "      <td>4447.689941</td>\n",
       "      <td>4447.689941</td>\n",
       "      <td>4471.370117</td>\n",
       "      <td>3000560000</td>\n",
       "      <td>4471.370117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>2021-10-18</td>\n",
       "      <td>4488.750000</td>\n",
       "      <td>4447.470215</td>\n",
       "      <td>4463.720215</td>\n",
       "      <td>4486.459961</td>\n",
       "      <td>2683540000</td>\n",
       "      <td>4486.459961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>4520.399902</td>\n",
       "      <td>4496.410156</td>\n",
       "      <td>4497.339844</td>\n",
       "      <td>4519.629883</td>\n",
       "      <td>2531210000</td>\n",
       "      <td>4519.629883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>4540.870117</td>\n",
       "      <td>4524.399902</td>\n",
       "      <td>4524.419922</td>\n",
       "      <td>4536.189941</td>\n",
       "      <td>2671560000</td>\n",
       "      <td>4536.189941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>2021-10-21</td>\n",
       "      <td>4551.439941</td>\n",
       "      <td>4526.890137</td>\n",
       "      <td>4532.240234</td>\n",
       "      <td>4549.779785</td>\n",
       "      <td>3016950000</td>\n",
       "      <td>4549.779785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>2021-10-22</td>\n",
       "      <td>4559.669922</td>\n",
       "      <td>4524.000000</td>\n",
       "      <td>4546.120117</td>\n",
       "      <td>4544.899902</td>\n",
       "      <td>3062810000</td>\n",
       "      <td>4544.899902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>2021-10-25</td>\n",
       "      <td>4572.620117</td>\n",
       "      <td>4537.359863</td>\n",
       "      <td>4553.689941</td>\n",
       "      <td>4566.479980</td>\n",
       "      <td>3250210000</td>\n",
       "      <td>4566.479980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>2021-10-26</td>\n",
       "      <td>4598.529785</td>\n",
       "      <td>4569.169922</td>\n",
       "      <td>4578.689941</td>\n",
       "      <td>4574.790039</td>\n",
       "      <td>2866500000</td>\n",
       "      <td>4574.790039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>4584.569824</td>\n",
       "      <td>4551.660156</td>\n",
       "      <td>4580.220215</td>\n",
       "      <td>4551.680176</td>\n",
       "      <td>3259510000</td>\n",
       "      <td>4551.680176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>4597.549805</td>\n",
       "      <td>4562.839844</td>\n",
       "      <td>4562.839844</td>\n",
       "      <td>4596.419922</td>\n",
       "      <td>3197560000</td>\n",
       "      <td>4596.419922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>2021-10-29</td>\n",
       "      <td>4608.080078</td>\n",
       "      <td>4567.589844</td>\n",
       "      <td>4572.870117</td>\n",
       "      <td>4605.379883</td>\n",
       "      <td>3632260000</td>\n",
       "      <td>4605.379883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>4620.339844</td>\n",
       "      <td>4605.120117</td>\n",
       "      <td>4610.620117</td>\n",
       "      <td>4606.600098</td>\n",
       "      <td>284628547</td>\n",
       "      <td>4606.600098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date         High          Low         Open        Close  \\\n",
       "2959  2021-10-05  4369.229980  4309.870117  4309.870117  4345.720215   \n",
       "2960  2021-10-06  4365.569824  4290.490234  4319.569824  4363.549805   \n",
       "2961  2021-10-07  4429.970215  4383.729980  4383.729980  4399.759766   \n",
       "2962  2021-10-08  4412.020020  4386.220215  4406.509766  4391.339844   \n",
       "2963  2021-10-11  4415.879883  4360.589844  4385.439941  4361.189941   \n",
       "2964  2021-10-12  4374.890137  4342.089844  4368.310059  4350.649902   \n",
       "2965  2021-10-13  4372.870117  4329.919922  4358.009766  4363.799805   \n",
       "2966  2021-10-14  4439.729980  4386.750000  4386.750000  4438.259766   \n",
       "2967  2021-10-15  4475.819824  4447.689941  4447.689941  4471.370117   \n",
       "2968  2021-10-18  4488.750000  4447.470215  4463.720215  4486.459961   \n",
       "2969  2021-10-19  4520.399902  4496.410156  4497.339844  4519.629883   \n",
       "2970  2021-10-20  4540.870117  4524.399902  4524.419922  4536.189941   \n",
       "2971  2021-10-21  4551.439941  4526.890137  4532.240234  4549.779785   \n",
       "2972  2021-10-22  4559.669922  4524.000000  4546.120117  4544.899902   \n",
       "2973  2021-10-25  4572.620117  4537.359863  4553.689941  4566.479980   \n",
       "2974  2021-10-26  4598.529785  4569.169922  4578.689941  4574.790039   \n",
       "2975  2021-10-27  4584.569824  4551.660156  4580.220215  4551.680176   \n",
       "2976  2021-10-28  4597.549805  4562.839844  4562.839844  4596.419922   \n",
       "2977  2021-10-29  4608.080078  4567.589844  4572.870117  4605.379883   \n",
       "2978  2021-11-01  4620.339844  4605.120117  4610.620117  4606.600098   \n",
       "\n",
       "          Volume    Adj Close  \n",
       "2959  2967400000  4345.720215  \n",
       "2960  3219590000  4363.549805  \n",
       "2961  3096080000  4399.759766  \n",
       "2962  2401890000  4391.339844  \n",
       "2963  2580000000  4361.189941  \n",
       "2964  2608150000  4350.649902  \n",
       "2965  2926460000  4363.799805  \n",
       "2966  2642920000  4438.259766  \n",
       "2967  3000560000  4471.370117  \n",
       "2968  2683540000  4486.459961  \n",
       "2969  2531210000  4519.629883  \n",
       "2970  2671560000  4536.189941  \n",
       "2971  3016950000  4549.779785  \n",
       "2972  3062810000  4544.899902  \n",
       "2973  3250210000  4566.479980  \n",
       "2974  2866500000  4574.790039  \n",
       "2975  3259510000  4551.680176  \n",
       "2976  3197560000  4596.419922  \n",
       "2977  3632260000  4605.379883  \n",
       "2978   284628547  4606.600098  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gspc = pd.read_csv(\"^GSPC.csv\")\n",
    "gspc.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T11:29:13.932598Z",
     "start_time": "2021-10-24T11:29:13.925871Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X,y = data.data, data.target\n",
    "X = pd.DataFrame(X)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(X, threshold = None):\n",
    "    counts = [len(unique(X[:,i])) for i in range(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T04:21:53.733730Z",
     "start_time": "2021-10-24T04:21:53.731366Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"C: {}\".format(fitted_svr_model.C))\n",
    "#     print(\"Epsilon: {}\".format(fitted_svr_model.epsilon))\n",
    "\n",
    "#     def param_list(self):\n",
    "#         \"\"\"\n",
    "#         The following function returns the list of parameters and the respective values.\n",
    "#         \"\"\"\n",
    "#         return {\"C\": self.C, \n",
    "#                 \"kernel\": self.kernel, \n",
    "#                 \"gamma\": self.gamma}\n",
    "\n",
    "#     def param_grid(self):\n",
    "        \n",
    "#         param_grid = {'C': [ 1, 10, 100, 1000,10000], \n",
    "#               'gamma': [1,0.1,0.01,0.001,0.0001],\n",
    "#               'kernel': ['rbf']}\n",
    "        \n",
    "#         pass\n",
    "#  if (kernel is None): # set rbf to default\n",
    "#             self.kernel = \"rbf\" \n",
    "#         else:\n",
    "#             self.kernel = kernel\n",
    "\n",
    "# print(\"Accuracy: {}%\".format(clf.score(X_test, y_test) * 100 ))\n",
    "#  if kernel == \"poly\":\n",
    "#             self.kernel = self._polynomial_kernel\n",
    "#             self.degree = degree\n",
    "#         elif kernel == \"rbf\":\n",
    "#             self.kernel = self._rbf_kernel\n",
    "#             self.sigma = sigma\n",
    "#         elif kernel = \"linear\":\n",
    "#             self.kernel = self_linear_kernel\n",
    "#         else: self.kernel = kernel\n",
    "\n",
    "# if self.C is None:\n",
    "#             self.C = C\n",
    "#         else:\n",
    "\n",
    "# import m as module\n",
    "\n",
    "# degree:int = 3,\n",
    "#                  gamma:float = 1.0,\n",
    "#                  coef0:float = 0.0,\n",
    "#                  C:float = 1.0,\n",
    "#                  epsilon:float = 0.1,\n",
    "#                  verbose:bool = False,\n",
    "#                  max_iter:int = 1000,\n",
    "#                  feature_names:Optional[list] = None):\n",
    "#     @decorator_function\n",
    "#     def create_Xy(self, X:float, y:float): # tells the reader that it should be float\n",
    "#         \"\"\"\n",
    "#         The below function sets the dependent and independent variables.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         X = np.array(\n",
    "#             t.bollinger_bands()[\n",
    "#                 [\"Bollinger Bands Lower\", \"Bollinger Bands Middle\", \"Bollinger Bands Upper\"]\n",
    "#                                 ]).reshape(-1,3)\n",
    "        \n",
    "#         y = np.array(ticker[\"Close\"]).reshape(len(ticker[\"Close\"]),1) # or -1   \n",
    "\n",
    "#def decorator_function(original_function):\n",
    "#        def wrapper_function(*args, **kwargs):\n",
    "#            print(\"wrapper executed this before {}\".format(original_function))\n",
    "#            return original_function(*args, **kwargs)\n",
    "#        return wrapper_function\n",
    "#    \n",
    "#    original_function = decorator_function(original_function)\n",
    "#    original_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
