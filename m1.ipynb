{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T11:17:12.366574Z",
     "start_time": "2021-10-24T11:17:11.451592Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import (StandardScaler,\n",
    "                                   MinMaxScaler,\n",
    "                                   Normalizer,\n",
    "                                   LabelEncoder\n",
    "                                  )\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_selection import (VarianceThreshold,\n",
    "                                       SelectKBest, mutual_info_classif, chi2,\n",
    "                                       RFE,\n",
    "                                       SelectFromModel,\n",
    "                                       SequentialFeatureSelector\n",
    "                                      )\n",
    "\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier, StackingClassifier)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     GridSearchCV,\n",
    "                                     cross_validate\n",
    "                                    )\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix,\n",
    "                             classification_report,\n",
    "                             accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score\n",
    "                            )\n",
    "\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import (check_array, \n",
    "                                      check_is_fitted, \n",
    "                                      check_X_y,\n",
    "                                      _check_sample_weight\n",
    "                                     )\n",
    "\n",
    "from sklearn.base import (BaseEstimator, \n",
    "                          TransformerMixin\n",
    "                         )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/mpalovic/Desktop\"\n",
    "ticker = \"gspc\"\n",
    "file_name = \"ta.{}\".format(str(ticker)) + \".csv\"\n",
    "df = pd.read_csv(filepath_or_buffer = \"{}/{}\".format(path, file_name), parse_dates=True, header=0, sep = \",\")\n",
    "x = pd.DataFrame(df.loc[:, df.columns!=\"Close\"])\n",
    "y = df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.columns.to_list()\n",
    "df.dtypes\n",
    "df.info()\n",
    "df.drop_duplicates()\n",
    "df.shape\n",
    "df.describe()\n",
    "df.isnull().sum()\n",
    "df.index\n",
    "df.nunique()\n",
    "df.sample(n=5)\n",
    "df[\"Date\"].value_counts()\n",
    "df.corr()\n",
    "df.astype(\n",
    "    {\"Volume\":float,\"On Balance Volume\":float}).dtypes\n",
    "df.loc[0:5, [\"Date\", \"Close\"]]\n",
    "df.sort_values(by=\"RSI\", ascending=False)\n",
    "df.size\n",
    "df.select_dtypes(\"object\").head(5)\n",
    "df.mask(df.isna(),0)\n",
    "\n",
    "mask = df[\"Close\"] >4200\n",
    "df[\"Close\"][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upside down dataframe with reset index\n",
    "df.loc[::-1].reset_index().head(3)\n",
    "df.loc[:,::-1].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_neg_vals(val):\n",
    "    color = \"red\" if val > 1400 else \"black\"\n",
    "    return \"color:%s\"%color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style with pandas dataframe\n",
    "df[\"Close\"].to_frame().style.applymap(color_neg_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T03:03:48.750681Z",
     "start_time": "2021-10-24T03:03:48.730188Z"
    }
   },
   "outputs": [],
   "source": [
    "class svm(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,\n",
    "                 estimator = \"SVC\",\n",
    "                 kernel:str = \"linear\",\n",
    "                 C:int = 1e3,\n",
    "                 learn_rate:float = 1e-3,\n",
    "                 tol:float = 0.05,\n",
    "                 batch_size:int = 1,\n",
    "                 n_epochs:int = 1000,\n",
    "                 decay:float = 1, \n",
    "                 random_number:float = None,\n",
    "                 visualisation = True,\n",
    "                 \n",
    "                 n = False\n",
    "                ):\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.kernel = self._kernel_type(kernel, **kwargs)\n",
    "        \n",
    "        self.C = C if C is not None\n",
    "        self.learn_rate = learn_rate\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.decay = decay\n",
    "        self.random_number = random_number if random_number is not None else np.random.randint(0,100,size=1)\n",
    "        \n",
    "        self.n = n\n",
    "        \n",
    "        self.w = False\n",
    "        self.b = False\n",
    "        \n",
    "        \n",
    "        self.visualisation = visualisation\n",
    "        self.colors = {1:\"r\", -1:\"b\"}\n",
    "        if self.visualisation:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1,1,1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def decorator_function(original_function):\n",
    "        def wrapper_function(*args, **kwargs):\n",
    "            print(\"executed before {} (original function).\".format(original_function.__name__))\n",
    "            return original_function(*args, **kwargs)\n",
    "        return wrapper_function\n",
    "    \n",
    "    \n",
    "    @decorator_function\n",
    "    def missing_vals(x, threshold = 0.6):\n",
    "        threshold = float(threshold)\n",
    "        for i in x.columns:\n",
    "            if isinstance(x,pd.DataFrame):\n",
    "                if float((x[i].isnull().sum()/x[i].shape[0])*100) > threshold:\n",
    "                    x_transformed = x.drop(labels = i, axis = 1)\n",
    "        return x_transformed\n",
    "    \n",
    "    \n",
    "    \n",
    "    def split(x,y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state = 101)\n",
    "\n",
    "        if x_train.shape[0] != x_test.shape[0]:\n",
    "            print(f\"\\n{x_test.shape[0]} obs in test set less than {x_train.shape[0]} obs in train set.\")\n",
    "\n",
    "        x_train.shape, y_train.shape\n",
    "        x_test.shape, y_test.shape\n",
    "\n",
    "        #x_train, x_test = np.array(x_train,dtype = np.float64), np.array(x_test,dtype = np.float64) \n",
    "\n",
    "        n_samples, n_features = x.shape\n",
    "        x_train, x_test = x_train.values.reshape(-1, n_features), x_test.values.reshape(-1, n_features)\n",
    "    \n",
    "        return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    def feature_scaling(x):\n",
    "        \n",
    "        n_samples, n_features = x.shape\n",
    "        x_train, x_test = train_test_split()\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        x_train = scaler.fit_transform(x_train.values.reshape(-1,n_features))\n",
    "        x_test = scaler.transform(x_test.values.reshape(-1,n_features))\n",
    "        \n",
    "        return x_train, x_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    def dimensionality_reduction(self, var_retained:float = .95, np:bool = False):\n",
    "        \n",
    "        x_train, x_test = feature_scaling()\n",
    "        pca = PCA(var_retained)\n",
    "        \n",
    "        x_train = pca.fit_transform(x_train)\n",
    "        x_test = pca.transform(x_test)\n",
    "        \n",
    "        if len(x_train.shape[1]) == len(n_features) and isinstance(x_train, np.float64):\n",
    "            raise ValueError(\n",
    "                \"%d n_features in X_train_pca is not less than %d n_features in X_train.\" \n",
    "                    (len(X_train_pca.shape[1]), len(n_features))\n",
    "            )\n",
    "                                                                    \n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "        return X_train, X_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def feature_selection(switch:bool):\n",
    "        if switch:\n",
    "            x_transformed = None\n",
    "            def variance_threshold(x_train):\n",
    "                if isinstance(x, pd.DataFrame):\n",
    "                    x = x.loc[:, x.columns!=\"Date\"]\n",
    "                    x.select_dtypes(include = np.number)\n",
    "        \n",
    "                    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "                    x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "                    selector = VarianceThreshold(.03)\n",
    "                    x_transformed = selector.fit_transform(x_scaled)\n",
    "                    x_transformed = pd.DataFrame(x_transformed)\n",
    "    \n",
    "                    print(f\"Original n_features: {x.shape[1]}.\")\n",
    "                    print(\"Transformed n_features: {}.\".format(x_transformed.shape[1]))\n",
    "    \n",
    "                    dropped = [col for col in x.columns if col not in x.columns[selector.get_support()]]\n",
    "                    dropped_list = [features for features in dropped]\n",
    "                    x_transformed.drop(dropped, axis = 1, inplace = True)\n",
    "        \n",
    "                return x_transformed\n",
    "        \n",
    "        else:\n",
    "            x_transformed = None\n",
    "            def selectKBest(x, n=3):\n",
    "                if isinstance(x, pd.DataFrame):\n",
    "                    x = x.loc[:, x.columns!=\"Date\"]\n",
    "                    x.select_dtypes(include = np.number)\n",
    "\n",
    "                    lab_enc = LabelEncoder()\n",
    "                    y_enc = lab_enc.fit_transform(y)\n",
    "\n",
    "                    selector = SelectKBest(score_func = mutual_info_classif, k = n)\n",
    "                    selector.fit(x,y_enc)\n",
    "\n",
    "                    a = [col for col in x.columns if col in x.columns[selector.get_support(True)]]\n",
    "                    x_transformed = x.loc[:,x.columns.isin(a)]\n",
    "        \n",
    "                    print(\"Original {}.\".format(x.shape))\n",
    "                    print(\"Transformed {}.\".format(x_transformed.shape))\n",
    "    \n",
    "                    #returns index where true\n",
    "                    z = {x.columns.get_loc(c): c for index,c in enumerate(x.columns[selector.get_support(True)])}\n",
    "    \n",
    "                return x_transformed\n",
    "           \n",
    "        return variance_threshold() if switch is not None else selectKBest()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        \n",
    "        x_train, x_test = split()\n",
    "        if x_train:\n",
    "        \n",
    "            n_samples, n_features = X.shape\n",
    "        \n",
    "            # if gamma is not specified in init, it is specified as\n",
    "            if not self.gamma:\n",
    "                self.gamma = 1/(n_features*X.var())\n",
    "        \n",
    "            if X_train.shape[1] != n_features:\n",
    "                raise ValueError(\"{} != {}\".format(X_train.shape[1], self.n_features))\n",
    "            elif X_train.shape[1] < 2:\n",
    "                raise ValueError(\"cannot fit model with {} features.\".format(X_train.shape[1]))\n",
    "            \n",
    "        \n",
    "            # Checks X and y for consistent length, enforces X to be 2D and y 1D. \n",
    "            # By default, X is checked to be non-empty and containing only finite values. \n",
    "            # Standard input checks are also applied to y, such as checking that y does not have np.nan or np.inf targets.\n",
    "            X, y = check_X_y(X, \n",
    "                             y, \n",
    "                             force_all_finite=False) # accepts np.nan in X\n",
    "        \n",
    "        \n",
    "            # By default, the input is checked to be a non-empty 2D array containing only finite values.\n",
    "            X = check_array(X, ensure_2d=True, ensure_min_samples=1, ensure_min_features=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            self.est = self._estimator(self.estimator)\n",
    "            self.est.fit(X_train, y_train)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if self.n:\n",
    "                y_train_pca_ = np.where(y_train_pca <= 0, -1,1)\n",
    "                n_samples, n_features = train.shape\n",
    "                \n",
    "                self.w = np.zeros(n_features) #each feature has to have some weight, in linear regression each beta parameter is a feature that has some weight\n",
    "                self.b = 0\n",
    "\n",
    "                if isinstance(self.C, type(None)):\n",
    "                    raise ValueError(\n",
    "                        \"Regularisation parameter %s not defined in the constructor method.\" (str(self.C))\n",
    "                    )\n",
    "                else: \n",
    "                    lambda_param = 1 / self.C\n",
    "            \n",
    "            \n",
    "            \n",
    "                self.n_iters = int(self.n_iters)\n",
    "                if self.n_iters <= 0:\n",
    "                    raise ValueError(\n",
    "                        f\"{self.n_iters} is less than zero.\"\n",
    "                    )\n",
    "        \n",
    "        \n",
    "        \n",
    "            #gradient descent\n",
    "            for _ in range(1, self.n_iters):\n",
    "                for index, x_i in enumerate(X.train_pca):\n",
    "                    condition = y_train_pca_[index] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                    if condition:\n",
    "                        # update params if condition true\n",
    "                        # parameter = parameter - (self.learning rate * gradient (derivative of cost function))\n",
    "                        self.w = self.w - (self.learn_rate * (2 * (1/self.C) * self.w))\n",
    "                    else:\n",
    "                        self.w = self.w - (self.learn_rate * ((2 * (1/self.C) * self.w) - np.dot(x_i, y_train_pca_[index])))\n",
    "                        self.b = self.b - (self.learn_rate * y_train_pca_[index])\n",
    "                        \n",
    "        return something if self.n is not None else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x.select_dtypes(include = np.number):\n",
    "    if np.any(np.isnan(x[i])) == True:\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(self):\n",
    "        \"\"\"The below returns the loss function for the stochastic gradient descent.\"\"\"\n",
    "        # The input to this function is the predicted output and the actual output.\n",
    "        pass\n",
    "        \n",
    "    def label_encoding(self, y:np.ndarray):\n",
    "        l = LabelEncoder()\n",
    "        y = l.fit_transform(y)\n",
    "        return y\n",
    "    \n",
    "if np:\n",
    "            # calculate cov matrix\n",
    "            cov_matrix = np.cov(X_train.T)\n",
    "            eigen_values, eigen_vectors = np.linalg.eig(cov_matrix)\n",
    "            \n",
    "            \n",
    "            # calculating explained variance on each component\n",
    "            var_expl = [i/(sum(eigen_values))*100 for i in eigen_values]\n",
    "            \n",
    "            # identifying components that explain at least 95% variance\n",
    "            cum_var_expl = np.cumsum(var_expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kernel_type(self, kernel:str = None, **kwargs):\n",
    "        \"\"\"Kernel is a hyperparameter and is selected by the researcher.\"\"\"\n",
    "        \n",
    "        if self.estimator == \"SVC\":\n",
    "        \n",
    "            if kernel:\n",
    "                if isinstance(kernel, str) and kernel is not None:\n",
    "                \n",
    "                    if kernel == \"linear\":\n",
    "                        def linear_kernel():\n",
    "                            return lambda X,y: np.dot(X,y.T)\n",
    "        \n",
    "                    if kernel == \"poly\":\n",
    "                        def _polynomial_kernel(bias = 0, power = 2):\n",
    "                            return lambda X,y: (self.gamma * np.dot(X,y)+bias)**power\n",
    "        \n",
    "                    if kernel == \"rbf\":\n",
    "                        def _rbf_kernel():\n",
    "                            return lambda X_i,y_i: np.exp(-self.gamma * np.dot(X_i-y_i, X_i-y_i))  \n",
    "        \n",
    "            kernel_mapping = {\n",
    "                \"linear\": _linear_kernel,\n",
    "                \"poly\": _polynomial_kernel,\n",
    "                \"rbf\": _rbf_kernel\n",
    "            }\n",
    "            \n",
    "            if kernel not in {None,\"linear\", \"poly\", \"rbf\"}:\n",
    "                raise ValueError(f\"{self.kernel} kernel not recognised.\")\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X_test:np.array, y = None):\n",
    "        \"\"\"Step 7: Model prediction.\"\"\"\n",
    "        if isinstance(X_test, np.array) and not y:\n",
    "            check_is_fitted(self, msg=\"is_fitted\")          \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        y_pred = self.base_regressor.predict(X_test)\n",
    "        \n",
    "        if self.n:\n",
    "            linear_output = np.sign(np.dot(np.array(n_features), self.w) + self.b)\n",
    "        \n",
    "        \n",
    "        return y_pred if self.n is False else linear output\n",
    "        \n",
    "        \n",
    "    \n",
    "    def evaluation(self):\n",
    "        \"\"\"The following function evaluates how well the model performs on the test data. Step 6: model evaluation.\"\"\"\n",
    "        print(\"\\nThe Classifier Accuracy Score is {:.2f}\\n\".format(clf.score(X_test, y_test)))\n",
    "        \n",
    "    def get_params(self, deep = True):\n",
    "        \"\"\"\n",
    "        The below function returns parameter values.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"C\": self.C,\n",
    "            \"kernel\": self.kernel\n",
    "            \"epsilon\": self.epsilon\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for param, val in params.items():\n",
    "            setattr(self, param, val)\n",
    "        return self    \n",
    "    \n",
    "    def gridSearchCV(self):\n",
    "        param_grid = {\n",
    "            \"C\": [1,10,100,1000,10000],\n",
    "            \"gamma\": [1,0.1,0.01,0.001,0.00001],\n",
    "            \"kernel\": [\"linear\", \"poly\", \"rbf\"],\n",
    "            \"class_weight\": [\"balanced\", None]\n",
    "        }\n",
    "        \n",
    "        search = GridSearchCV(estimator = svm, \n",
    "                             param_grid = param_grid, \n",
    "                             cv = 5, # determines cross-validation splitting strategy, int, specify number of folds in StratifiedKfold\n",
    "                             verbose = 1, # control verbosity, the higher the more messages\n",
    "                             refit = True, # refit an estimator using the best found params on data\n",
    "                             scoring = accuracy)\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        y_pred = search.predict(X_test)\n",
    "        \n",
    "        print(\"Test Accuracy: {}.\".format(accuracy_score(y_test, y_pred)))\n",
    "        print(\"Best Params: {}.\".format(model.best_params_))\n",
    "        \n",
    "    def main():\n",
    "        f = svm()\n",
    "        \n",
    "        f.load_data()\n",
    "        f.split()\n",
    "        f.feature_scaling()\n",
    "        f.dimensionality_reduction()\n",
    "        f.variance_threshold()\n",
    "        f.selectKBest()\n",
    "        \n",
    "        f.fit()\n",
    "        f.predict()\n",
    "        \n",
    "    if __name__ = \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(a,b,q):\n",
    "    if q:\n",
    "        e = None\n",
    "        def plus(a,b):\n",
    "            e = a+b\n",
    "            return \"Ahoj volam sa martin\"\n",
    "    else:\n",
    "        E = None\n",
    "        def minus(a,b):\n",
    "            E = a-b\n",
    "            return E\n",
    "    return plus(a,b) if q is not None else minus(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(40,10, q = \"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
