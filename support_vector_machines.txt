Support Vector Machines: 

The support vector machine (or SVMs) is a supervised machine learning model that can be used for regression and classification problems. 
It is used to separate two classes of data points. 
The objective is to find a hyperplane that has the maximum margin, i.e. maximum distance between data points of the two classes. 
Finding the maximum margin means that future data points can be classified with more confidence. 

Hyperplane can be thought as a decision boundary that helps us to classify data points. 

The shape of the hyperplane depends on the number of features (columns) in our data. 
If the number of features is two, then the shape of the hyperplane is a linear line. 
If the number of features is three, then the shape of the hyperplane is a two-dimensional plane. 
It is difficult to imagine the hyperplane when there are more than three features in our data. 

Support vectors are data points that are closer to the hyperplane and influence the position of such hyperplane (hence "support"). 
They are actually data points chosen by the researcher. 
The data points, or support vectors, are chosen such as to maximise margin of the classifier (the width/the distance between the data points of two classes). 
Deleting these data points will change the position of hyperplane. These data points are the points that allows us to build the SVM. 

Maximum-margin intuition:
In logistic regression, the output of the linear function is scaled (0,1) using sigmoid function. 
If the scaled value is greater than 0.5 (threshold), then label 1 is assigned, else 0 is assigned. 
In SVM, if the output of the linear function is greater than 1, a class is assigned, and of less than -1, other class is assigned. The threshold values are (-1,1) in SVM. 

Step-by-step process: 
1) data pre processing (scale the data and create x,y)
2) train the model
3) prediction
4) classification report/confusion matrix 
5) gridsearchcv (prediction with gridsearchcv)

The most important parameters are: 
1) "C"
- controls the cost of misclassification on the training data
- 






2) "gamma"
3) "kernel"

The SVM model does not provide probability estimates, but they can be calculated five-fold cross-validation (see scikit-learn).

Kernel trick is a method to solve a non-linear problem using a linear classifier. 