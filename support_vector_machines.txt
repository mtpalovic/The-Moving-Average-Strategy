Support Vector Machines: 

The support vector machine (or SVMs) is a supervised machine learning model that can be used for regression and classification problems. 
It is used to separate two classes of data points. 
The objective is to find a hyperplane that has the maximum margin, i.e. maximum distance between data points of the two classes. 
Finding the maximum margin means that future data points can be classified with more confidence. 

Hyperplane can be thought as a decision boundary that helps us to classify data points. 

The shape of the hyperplane depends on the number of features (columns) in our data. 
If the number of features is two, then the shape of the hyperplane is a linear line. 
If the number of features is three, then the shape of the hyperplane is a two-dimensional plane. 
It is difficult to imagine the hyperplane when there are more than three features in our data. 

Support vectors are data points that are closer to the hyperplane and influence the position of such hyperplane (hence "support"). 
They are actually data points chosen by the researcher. 
The data points, or support vectors, are chosen such as to maximise margin of the classifier (the width/the distance between the data points of two classes). 
Deleting these data points will change the position of hyperplane. These data points are the points that allows us to build the SVM. 

Maximum-margin intuition:
In logistic regression, the output of the linear function is scaled (0,1) using sigmoid function. 
If the scaled value is greater than 0.5 (threshold), then label 1 is assigned, else 0 is assigned. 
In SVM, if the output of the linear function is greater than 1, a class is assigned, and of less than -1, other class is assigned. The threshold values are (-1,1) in SVM. 

Step-by-step process: 
1) data pre processing (scale the data and create x,y)
2) train the model
3) prediction
4) classification report/confusion matrix 
5) gridsearchcv (prediction with gridsearchcv)

The most important parameters are: 
1) "C"
- controls the cost of misclassification on the training data
- small c/small variance, and high bias/high margin hyperplane (similar to linear model)
- high c/high variance, and small bias/small margin hyperplane (similar to complex model)

- small c does not penalise the cost of misclassification much (searcher for high margin hyperplane even if it penalises more data points)
- high c does penalise the cost of misclassification (small margin hyperplane so that all data points are classified correctly), high c might cause the model to overfit


2) "gamma"
3) "kernel"

Bias-variance tradeoff: (compare train/test data)
Linear model 	- high bias (difference between predicted and expected value, sum of SSR)
		- low variance (roughly identical linear line between train/test set)

Complex model 	- low bias (squiggly line fits data too well, overfitting in train, poor results in test)
		- high variance (very different squiggly line in train/test data) 	

The common method for finding the best tradeoff between bias/variance is regularisation ("C" parameter in SVM). 

Notes:
Kernel trick

